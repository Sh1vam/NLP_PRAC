{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa92cf9c-eb06-4310-953f-cab040ec896d",
   "metadata": {},
   "source": [
    "# Practical 1\n",
    "## Introduction to python libraries for feature extraction and NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2fd618-e3ff-4825-b3b8-2c3cfc94f02d",
   "metadata": {},
   "source": [
    "### Feature extraction is a crucial part of any Natural Language Processing (NLP) project, as it transforms raw text into numerical representations that machine learning models can work with. Python offers several libraries that make this task easier, with each library providing tools to handle different aspects of feature extraction, text processing, and NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6653746e-0cac-415b-9698-b2289a5588f8",
   "metadata": {},
   "source": [
    "## 1. NLTK (Natural Language Toolkit)\n",
    "### NLTK is one of the most popular libraries for working with human language data (text). It provides various tools for tokenization, stemming, tagging, parsing, and semantic reasoning. It also includes a suite of text corpora to help build models.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "* Tokenization: Breaking text into words or sentences.\n",
    "* Stemming: Reducing words to their root forms.\n",
    "* POS Tagging: Part-of-Speech tagging.\n",
    "* Chunking: Dividing text into syntactically correlated parts.\n",
    "* Corpora Access: Pre-defined datasets like WordNet, Gutenberg, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20dd94d2-c113-4230-9e8b-7140b892a01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', 'with', 'Python', 'is', 'amazing', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PCD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Natural language processing with Python is amazing.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a94e0-d783-4d85-8c5c-e99562a1d85e",
   "metadata": {},
   "source": [
    "## 2. spaCy\n",
    "### spaCy is a more modern and efficient library for NLP tasks. It focuses on deep learning integration and supports large-scale processing of text. It offers pre-trained models for multiple languages and is faster than NLTK in some operations.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "* Named Entity Recognition (NER): Identifies real-world entities.\n",
    "* Dependency Parsing: Analyzing relationships between words.\n",
    "* Tokenization: Split text into tokens.\n",
    "* Pre-trained models: Ready-to-use models for language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95ff733c-87b4-428f-9593-f9bd261dac8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural \t\t ADJ \t JJ \t\t\t adjective (English), other noun-modifier (Chinese)\n",
      "language \t\t NOUN \t NN \t\t\t noun, singular or mass\n",
      "processing \t\t NOUN \t NN \t\t\t noun, singular or mass\n",
      "with \t\t ADP \t IN \t\t\t conjunction, subordinating or preposition\n",
      "Python \t\t PROPN \t NNP \t\t\t noun, proper singular\n",
      "is \t\t AUX \t VBZ \t\t\t verb, 3rd person singular present\n",
      "amazing \t\t ADJ \t JJ \t\t\t adjective (English), other noun-modifier (Chinese)\n",
      ". \t\t PUNCT \t . \t\t\t punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"Natural language processing with Python is amazing.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text,\"\\t\\t\",token.pos_,\"\\t\",token.tag_,\"\\t\\t\\t\",spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82fd0ba-26d5-4158-ad39-a93d6787db02",
   "metadata": {},
   "source": [
    "## 3. Scikit-learn\n",
    "### Scikit-learn is not a dedicated NLP library, but it provides many useful tools for feature extraction from text. It includes methods for converting text into numerical features like Bag-of-Words (BoW) and TF-IDF representations.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "* CountVectorizer: Converts a collection of text documents to a matrix of token counts (BoW).\n",
    "* TfidfVectorizer: Converts a collection of text documents to a matrix of TF-IDF features.\n",
    "* Text classification: Directly used for training ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "875317b1-d17e-4f83-b61e-12038e100970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for' 'great' 'is' 'language' 'learning' 'machine' 'natural' 'processing'\n",
      " 'python' 'with']\n",
      "[[0 0 0 1 0 0 1 1 1 1]\n",
      " [1 1 1 0 1 1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "docs = [\"Natural language processing with Python.\", \n",
    "        \"Python is great for machine learning.\"]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5950d18a-e20e-4290-93ab-a454744acb44",
   "metadata": {},
   "source": [
    "## 4. Gensim\n",
    "### Gensim is used for topic modeling and document similarity analysis. It's designed to handle large text corpora by providing efficient algorithms like Word2Vec for word embeddings.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "* Topic modeling: Latent Dirichlet Allocation (LDA).\n",
    "* Word2Vec: Creating dense word embeddings.\n",
    "* Document similarity: Calculate similarity between documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c95d989-722d-4f30-8169-daf9c70d4f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00713902  0.00124103 -0.00717672 -0.00224462  0.0037193   0.00583312\n",
      "  0.00119818  0.00210273 -0.00411039  0.00722533 -0.00630704  0.00464722\n",
      " -0.00821997  0.00203647 -0.00497705 -0.00424769 -0.00310898  0.00565521\n",
      "  0.0057984  -0.00497465  0.00077333 -0.00849578  0.00780981  0.00925729\n",
      " -0.00274233  0.00080022  0.00074665  0.00547788 -0.00860608  0.00058446\n",
      "  0.00686942  0.00223159  0.00112468 -0.00932216  0.00848237 -0.00626413\n",
      " -0.00299237  0.00349379 -0.00077263  0.00141129  0.00178199 -0.0068289\n",
      " -0.00972481  0.00904058  0.00619805 -0.00691293  0.00340348  0.00020606\n",
      "  0.00475375 -0.00711994  0.00402695  0.00434743  0.00995737 -0.00447374\n",
      " -0.00138926 -0.00731732 -0.00969783 -0.00908026 -0.00102275 -0.00650329\n",
      "  0.00484973 -0.00616403  0.00251919  0.00073944 -0.00339215 -0.00097922\n",
      "  0.00997913  0.00914589 -0.00446183  0.00908303 -0.00564176  0.00593092\n",
      " -0.00309722  0.00343175  0.00301723  0.00690046 -0.00237388  0.00877504\n",
      "  0.00758943 -0.00954765 -0.00800821 -0.0076379   0.00292326 -0.00279472\n",
      " -0.00692952 -0.00812826  0.00830918  0.00199049 -0.00932802 -0.00479272\n",
      "  0.00313674 -0.00471321  0.00528084 -0.00423344  0.0026418  -0.00804569\n",
      "  0.00620989  0.00481889  0.00078719  0.00301345]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [[\"I\", \"love\", \"natural\", \"language\", \"processing\"],\n",
    "             [\"Python\", \"is\", \"great\", \"for\", \"NLP\"]]\n",
    "\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "vector = model.wv['Python']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3268ad42-e96b-42eb-a1c8-780d23135ffa",
   "metadata": {},
   "source": [
    "## 5. Transformers (Hugging Face)\n",
    "### Transformers from Hugging Face is a powerful library for working with pre-trained transformer models like BERT, GPT, and RoBERTa. It provides tools for text classification, question answering, and text generation.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "* Pre-trained transformer models.\n",
    "* Tokenization.\n",
    "* Text generation.\n",
    "* Text classification.\n",
    "* Question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "664a8b9c-b735-4484-9004-d349ed84a26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4afa7194be5141acad200868d43f41d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Sem 7\\NLP\\Practicals\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\PCD\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5fcb3c000b43d5b8662cff45ebd62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f1da8079604a68b7e6e70f0c45e597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0c7c7523cd47638ac880bc862e9039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Sem 7\\NLP\\Practicals\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998145699501038}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(\"sentiment-analysis\")\n",
    "result = nlp(\"Natural Language Processing is amazing with Python!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f699da-a1b0-49d2-869f-0ab68599de2e",
   "metadata": {},
   "source": [
    "## 6. TextBlob\n",
    "### TextBlob is a simpler library built on top of NLTK and Pattern. It offers an easy-to-use API for common NLP tasks like part-of-speech tagging, noun phrase extraction, and sentiment analysis.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "* Part-of-Speech Tagging.\n",
    "* Noun Phrase Extraction.\n",
    "* Sentiment Analysis.\n",
    "* Translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4518249b-1777-444a-a794-29b32b7e1290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.2, subjectivity=0.30000000000000004)\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"Natural Language Processing is fun.\"\n",
    "blob = TextBlob(text)\n",
    "print(blob.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8364adb1-c08a-47bd-bbc8-94ddcfb69c14",
   "metadata": {},
   "source": [
    "\n",
    "> In TextBlob, sentiment analysis returns two main components:\n",
    "\n",
    "- Polarity: A value between -1 and 1 that tells how positive or negative a sentence is. A score closer to 1 means positive sentiment, while a score closer to -1 means negative sentiment.\n",
    "- Subjectivity: A value between 0 and 1 that tells how subjective (personal opinion, feeling) or objective (fact-based) the sentence is. A score closer to 1 means high subjectivity.\n",
    "  \n",
    "> In the case of Sentiment(polarity=0.2, subjectivity=0.30000000000000004), the breakdown is as follows:\n",
    "\n",
    "* Polarity = 0.2: The text is slightly positive, but not strongly so. It's more neutral with a slight tilt toward positive sentiment.\n",
    "* Subjectivity = 0.3: The text is mostly objective, meaning it is more fact-based, with a bit of personal opinion or subjectivity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ea7bb6-17fc-42ad-a430-e70820df5d05",
   "metadata": {},
   "source": [
    "## 7. TfidfTransformer (Scikit-learn)\n",
    "### TfidfTransformer is another Scikit-learn feature extraction tool that converts a matrix of token counts into TF-IDF scores, offering better understanding of the importance of words in documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e87ed8f3-e4e1-4fa7-88a5-d69cddf20f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazing' 'for' 'is' 'love' 'nlp' 'python' 'tasks']\n",
      "[[0.         0.         0.         0.81480247 0.         0.57973867\n",
      "  0.        ]\n",
      " [0.4261596  0.4261596  0.4261596  0.         0.4261596  0.30321606\n",
      "  0.4261596 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "docs = [\"I love Python.\", \"Python is amazing for NLP tasks.\"]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_matrix = tfidf_transformer.fit_transform(X)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ab2e5d-7f90-40c4-a71d-7fd86863c3a6",
   "metadata": {},
   "source": [
    "### Document 1: \"I love Python.\"\n",
    "#### Row 1: [0. 0. 0. 0.81480247 0. 0.57973867 0. ]\n",
    "-> The TF-IDF scores for words in Document 1:\n",
    "* 'amazing' → 0 (not present)\n",
    "* 'for' → 0 (not present)\n",
    "* 'love' → 0 (not present)\n",
    "* 'nlp' → 0.8148 (high score, probably due to its rarity)\n",
    "* 'python' → 0.5797 (present and somewhat significant in Document 1)\n",
    "* 'tasks' → 0 (not present)\n",
    "### Document 2: \"Python is amazing for NLP tasks.\"\n",
    "#### Row 2: [0.4261596 0.4261596 0.4261596 0. 0.4261596 0.30321606 0.4261596 ]\n",
    "-> The TF-IDF scores for words in Document 2:\n",
    "* 'amazing' → 0.4261 (present and fairly important)\n",
    "* 'for' → 0.4261 (present and fairly important)\n",
    "* 'love' → 0.4261 (appears in this document)\n",
    "* 'nlp' → 0 (not present)\n",
    "* 'python' → 0.4261 (present but appears in both documents)\n",
    "* 'tasks' → 0.3032 (present but less important)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3632d1d4-2a34-42d2-bbb8-ddb0cb65a2f3",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "#### These libraries together cover the essential aspects of feature extraction and NLP tasks. You can choose based on your project’s needs: for large-scale processing (spaCy), deep learning models (Transformers), traditional ML (Scikit-learn), or specific NLP tasks (NLTK)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
